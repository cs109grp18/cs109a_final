{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of our sources, we gathered data for the years 2006 - 2017. Our observational unit was **Metropolitan Statistical Area (MSA).** An MSA is defined to be a geographical area with high population density.\n",
    "\n",
    "**Data Source 1: FBI Uniform Crime Reporting (UCR) Program**\n",
    "\n",
    "We got our data on murder and crime from the FBI database for the years 2006 - 2017. **Table 1** lists the features provided in the FBI database.\n",
    "\n",
    "|          Name          |\n",
    "|--------------------------------|\n",
    "| Violent Crime                     |\n",
    "| Murder + Non-negligent Manslaughter|\n",
    "| Forcible Rape               |\n",
    "| Robbery                  |\n",
    "| Aggravated Assault            |\n",
    "| Property Crime           |\n",
    "| Burglary           |\n",
    "| Larceny-theft                |\n",
    "| Motor Vehicle Theft                |\n",
    "| Population of MSA         |\n",
    "| Population of Largest city within MSA          |\n",
    "\n",
    "The crime variables available from Table 1 were available in three separate forms: number for entire MSA, number within the largest city of the MSA, and the number across the MSA per 100,000 individuals (a rate).\n",
    "\n",
    "**Data Source 2: United States Census Bureau (UCSB)**\n",
    "\n",
    "We obtained demographic data for each MSA from the USCB.  We used our EDA, intuition, and the provided Glaeser paper to collect features. Table 2 list the names, definitions, and table numbers for which we got census data.\n",
    "\n",
    "\n",
    "|          Feature Name          | Feature Definition | USCB Table Number |\n",
    "|--------------------------------|---|----------------------------------------------------------------------|\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "| Placeholder 1                     | Description 1 | Table 1 |\n",
    "\n",
    "\n",
    "We selected these variables because data were widely available (by year and by MSA) and we wanted to try to collect a range of economic, gender, racial, and age features to get a diverse array of variables about an MSA.\n",
    "\n",
    "**Data Source 3: Bureau of Economic Analysis (BEA) Real PC GDP**\n",
    "\n",
    "The data on income from UCSB were mostly missing and thus unusable.  Instead, we looked to the BEA which provided real per-capita GDP, which can be seen as a metric to measure an MSA aggregate economic activity.  The reason we chose real per-capita GDP are two-fold. Per-capita GDP is better than GDP for our purposes because it adjusts for population. Otherwise, larger MSA will always have larger GDP due to more people and resources. Secondly, real per capita GDP was chosen to control for inflation. Our GDP measure is in 2005 dollars for all years so that comparisons between years is due to actual changes rather than inflation\n",
    "\n",
    "URL: https://www.bea.gov/iTable/iTable.cfm?ReqID=70#reqid=70&step=1&isuri=1&7003=1000&7004=naics&7035=-1&7005=1&7006=xx&7001=21000&7036=-1&7002=2&7090=70&7007=-1&7093=levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FBI and Crime data did not have a numeric ID for MSA.  So, we created a unique “join_key” for each MSA.  The structure of MSA is as follows “Largest City within MSA-Second Largest City within MSA-etc-State”. For example, here is one MSA: “Baltimore-Columbia-Townson, MD”. To create the join key, we grabbed the first city name (largest city) and concatenated it with the state to create “Baltimore-MD”. \n",
    "\n",
    "We then used this “join_key” along with year to merge the three data-sets to create one complete data frame that we used for EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "---------------\n",
    "split_MSA\n",
    "\n",
    "This method takes in a dataframe with MSA and splits into a city_key (largest city)\n",
    "and state_key. This will help facilitate MSA merging\n",
    "\n",
    "Returns dataframe with these two additional features\n",
    "\"\"\"\n",
    "def split_MSA(df):\n",
    "    df['MSA'] = df['MSA'].str.replace('Metro Area', '')\n",
    "    # Need to manually fix how this MSA is written\n",
    "    df.loc[df['MSA'].str.contains(\"Texarkana\"), \"MSA\"] = \"Texarkana, AR-TX\"\n",
    "\n",
    "    #Grab Everything before comma\n",
    "    df['city_key'] = df['MSA'].str.split(\",\").str[0]\n",
    "    # Then grab everything before first hyphen if it has it\n",
    "    df['city_key'] = df['city_key'].str.split(\"-\").str[0].str.strip()\n",
    "    # State will be everying after comma \n",
    "    df['state_key']=df['MSA'].str.split(\",\").str[1].str.strip()\n",
    "    return(df)\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "append_df\n",
    "\n",
    "This function appends two dataframes\n",
    "\n",
    "Parameters:\n",
    "    input - dataframe to be appended\n",
    "    output - dataframe to be appended onto\n",
    "    \n",
    "Returns a single dataframe \n",
    "\"\"\"\n",
    "def append_df(input,output):\n",
    "    if output.empty:\n",
    "        output=input.copy()\n",
    "    else:\n",
    "        output=pd.concat([output,input])\n",
    "        output.reset_index(drop='Index',inplace=True)\n",
    "    return(output)\n",
    "\n",
    "'''\n",
    "Function\n",
    "-----------\n",
    "var_thresh\n",
    "\n",
    "This function takes in a dataframe and keeps only those varaibles that have a pct\n",
    "non-missing that is above that threshold\n",
    "'''\n",
    "def var_thresh(df, thresh=0.65):\n",
    "    return(df.loc[:, pd.notnull(df).sum() > len (df) *thresh])\n",
    "\n",
    "'''\n",
    "Function\n",
    "---------\n",
    "slim_df\n",
    "\n",
    "This function takes in a list of variables to keep\n",
    "on the the given df. It keep the variables + geography\n",
    "then renames to MSA and drops the first row of variable descriptions\n",
    "'''\n",
    "def slim_df(df, var_list):\n",
    "    var_list.append('GEO.display-label')\n",
    "    df = df.loc[:, var_list]\n",
    "    # Get rid of Micro Areas\n",
    "    df = df.loc[~df['GEO.display-label'].str.contains(\"Micro Area\"), :]\n",
    "    \n",
    "    df = df.rename(index=str, columns={'GEO.display-label': 'MSA'})\n",
    "    df['MSA'] = df[\"MSA\"].astype(str)\n",
    "    # Drop first row of var descriptions\n",
    "    df = df.loc[df.MSA != \"Geography\", :]\n",
    "    # Split MSA into city-state key\n",
    "    return(split_MSA(df))\n",
    "\n",
    "'''\n",
    "Function\n",
    "---------\n",
    "match_crime\n",
    "\n",
    "This function will take in a dataframe and make changes to MSA\n",
    "in order to match crime data\n",
    "'''\n",
    "def match_crime(df):\n",
    "    df.loc[df['MSA'].str.contains('Crestview'),'city_key']='Crestview'\n",
    "    df.loc[df['MSA'].str.contains('Sarasota'),'city_key']='North Port'\n",
    "    df.loc[df['MSA'].str.contains('Louisville'),'city_key']='Louisville'\n",
    "    df.loc[df['MSA'].str.contains('Santa Maria'),'city_key']='Santa Maria'\n",
    "    df.loc[df['MSA'].str.contains('Weirton'),'city_key']='Weirton'\n",
    "    df.loc[df['MSA'].str.contains('San Germán'),'city_key']='San German'\n",
    "    df.loc[df['MSA'].str.contains('Mayagüez'),'city_key']='Mayaguez'\n",
    "    df.loc[df['MSA'].str.contains('Honolulu'),'city_key']='Urban Honolulu'\n",
    "\n",
    "    #State\n",
    "    df.loc[df['MSA'].str.contains('Worcester'),'state_key']='MA-CT'\n",
    "    df.loc[df['MSA'].str.contains('Myrtle Beach'),'state_key']='SC-NC'\n",
    "    df.loc[df['MSA'].str.contains('Salisbury'),'state_key']='MD-DE'\n",
    "    df.loc[df['MSA'].str.contains('Weirton'),'state_key']='WV-OH'\n",
    "    return(df)\n",
    "\n",
    "'''\n",
    "Function\n",
    "--------\n",
    "get_file_name\n",
    "\n",
    "Get the appropriate file name giving year and table code\n",
    "\n",
    "'''\n",
    "def get_file_name(year, table_code):\n",
    "    if year == 2006:\n",
    "        mid = 'EST'\n",
    "    else:\n",
    "        mid = '1YR'\n",
    "    return('ACS_'+str(year)[2:]+\"_%s_\" %mid + table_code)\n",
    "\n",
    "'''\n",
    "Function\n",
    "--------\n",
    "convert_to_int\n",
    "\n",
    "This function takes in a dataframe and list of vars to convert to int\n",
    "'''\n",
    "def convert_to_int(df, int_vars):\n",
    "    df[int_vars] = df[int_vars].astype(int)\n",
    "    return(df)\n",
    "'''\n",
    "Function\n",
    "----------\n",
    "create_proportions\n",
    "\n",
    "This function will take in a list of variables and a single total variable\n",
    "It then creates proportions by dividing each of the variables in the list by the total\n",
    "to create a proportion\n",
    "'''\n",
    "def create_proportions(df,num_list, total_var):\n",
    "    df.loc[:, num_list] = df[num_list].apply(lambda x: x / df[total_var])\n",
    "    del df[total_var]\n",
    "    return(df)\n",
    "\n",
    "\"\"\"\n",
    "function\n",
    "-----------\n",
    "fbi_url_generator\n",
    "\n",
    "This function pulls violent crime spreadsheets from FBI UCR website\n",
    "for a given year\n",
    "\n",
    "It takes in the year of interest and outputs a url string\n",
    "\"\"\"\n",
    "def fbi_url_generator(year):\n",
    "    if 2006 <= year <= 2009:\n",
    "        return('https://www2.fbi.gov/ucr/cius%i/data/documents/'%year +str(year)[2:]+'tbl06.xls')\n",
    "    else:\n",
    "        if 2010 <= year <= 2011:\n",
    "            end = '/tables/table-6/output.xls'\n",
    "        elif 2012 <= year <= 2013:\n",
    "            end = '/tables/6tabledatadecpdf/table-6/output.xls'\n",
    "        elif 2014 <= year <= 2015:\n",
    "            if year == 2014:\n",
    "                mid = 'Table_6_Crime_in_the_United_States_by_Metropolitan_Statistical_Area_2014/output.xls'\n",
    "            else:\n",
    "                mid = 'table_6_crime_in_the_united_states_by_metropolitan_statistical_area_%i.xls/output.xls' %year\n",
    "            end = '/tables/table-6/%s' %mid\n",
    "        elif year == 2016:\n",
    "            end ='/tables/table-4/table-4/output.xls' \n",
    "        hostname = 'https://ucr.fbi.gov/crime-in-the-u.s/%i/crime-in-the-u.s.-%i' %(year, year)\n",
    "        return(hostname + end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/employ/ACS_06_EST_S2301.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b49e2f95ff69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2006\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2006\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'S2301'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0memploy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/employ/%s.csv\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Grab Unemployment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/employ/ACS_06_EST_S2301.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Employment Data\n",
    "#####################\n",
    "emp_all = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    f = get_file_name(2006, 'S2301')\n",
    "    employ = pd.read_csv(\"data/employ/%s.csv\" %f, encoding='Latin-1')\n",
    "    \n",
    "    # Grab Unemployment\n",
    "    un = [v for v in employ.columns if \"HC04\" in v and \"EST\" in v]\n",
    "    employ = slim_df(employ, un)\n",
    "    \n",
    "    employ = employ.loc[:, [\"MSA\", \"city_key\", \"state_key\", \n",
    "                          \"HC04_EST_VC01\", \"HC04_EST_VC03\",\n",
    "                         'HC04_EST_VC24']]\n",
    "    employ['year'] = year\n",
    "    emp_all = append_df(employ, emp_all) \n",
    "\n",
    "# Process Final DataFrame\n",
    "emp_all = emp_all.sort_values(['city_key', 'state_key', 'year'])\n",
    "emp_all = match_crime(emp_all)\n",
    "del emp_all['MSA']\n",
    "emp_all = emp_all.rename(index=str,\n",
    "                        columns={'HC04_EST_VC01': 'unemp_16_ovr',\n",
    "                                'HC04_EST_VC03': 'unemp_16_19',\n",
    "                                'HC04_EST_VC24': 'unemp_female'})\n",
    "emp_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Age Data\n",
    "############\n",
    "age_all = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    f = get_file_name(year, 'S0101')\n",
    "    age = pd.read_csv(\"data/age/%s.csv\" %f, encoding='Latin-1')\n",
    "    age = slim_df(age, [v for v in age.columns if \"EST\" in v])\n",
    "    age = age.replace(\"(X)\", np.nan)\n",
    "\n",
    "    age = age.loc[:, ['MSA','city_key','state_key',\n",
    "                      'HC01_EST_VC33','HC01_EST_VC34',\n",
    "                      'HC01_EST_VC01', 'HC02_EST_VC01',\n",
    "                      'HC03_EST_VC01', 'HC01_EST_VC06',\n",
    "                      'HC01_EST_VC07', 'HC02_EST_VC07']]\n",
    "    age['year'] = year\n",
    "    age_all = append_df(age, age_all) \n",
    "\n",
    "\n",
    "# Process Final DataFrame\n",
    "age_all = age_all.sort_values(['city_key', 'state_key', 'year'])\n",
    "age_all = age_all.rename(index=str,\n",
    "                         columns={'HC01_EST_VC33':'median_age',\n",
    "                                'HC01_EST_VC34': 'sex_ratio',\n",
    "                                'HC01_EST_VC01': 'total_pop',\n",
    "                                'HC02_EST_VC01': 'male_pop',\n",
    "                                'HC03_EST_VC01': 'female_pop',\n",
    "                                'HC01_EST_VC06': 'pop_15_19',\n",
    "                                'HC01_EST_VC07': 'pop_20_24',\n",
    "                                'HC02_EST_VC07': 'male_pop_20_24'})\n",
    "\n",
    "# Convert to Int and Get Proportions\n",
    "age_all = convert_to_int(age_all, ['total_pop', 'male_pop', 'female_pop'])\n",
    "age_all = create_proportions(age_all, ['male_pop', 'female_pop'], 'total_pop')\n",
    "# Match Crime Data and then get rid of MSA\n",
    "age_all = match_crime(age_all)\n",
    "del age_all['MSA']\n",
    "age_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Income Data\n",
    "###############\n",
    "inc_all = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    f = get_file_name(year, 'B19001F')\n",
    "    inc = pd.read_csv(\"data/house_income/%s.csv\" %f, encoding='Latin-1')\n",
    "    # Keep only the estimates\n",
    "    inc = slim_df(inc, [v for v in inc.columns if \"HD01\" in v])\n",
    "    inc['year'] = year\n",
    "    inc_all = append_df(inc, inc_all) \n",
    "\n",
    "# Proccess Final Data Frame\n",
    "inc_all =  inc_all.rename(index=str,\n",
    "                          columns={'HD01_VD01':'total',\n",
    "                                  'HD01_VD02': 'inc_lt10',\n",
    "                                  'HD01_VD03': 'inc_10_15',\n",
    "                                  'HD01_VD04': 'inc_15_19',\n",
    "                                  'HD01_VD05': 'inc_20_24',\n",
    "                                  'HD01_VD06': 'inc_25_29',\n",
    "                                  'HD01_VD07': 'inc_30_34',\n",
    "                                  'HD01_VD08': 'inc_35_39',\n",
    "                                  'HD01_VD09': 'inc_40_44',\n",
    "                                  'HD01_VD10': 'inc_45_49',\n",
    "                                  'HD01_VD11': 'inc_50_59',\n",
    "                                  'HD01_VD12': 'inc_60_74',\n",
    "                                  'HD01_VD13':'inc_75_99',\n",
    "                                  'HD01_VD14':'inc_100_124',\n",
    "                                  'HD01_VD15':'inc_125_149',\n",
    "                                  'HD01_VD16':'inc_150_199',\n",
    "                                  'HD01_VD17':'inc_gt_200'})\n",
    "\n",
    "numeric_vars =  [v for v in inc_all.columns if \"inc\" in v]\n",
    "inc_all = convert_to_int(inc_all, numeric_vars)\n",
    "inc_all['total'] = inc_all['total'].astype(int)\n",
    "# Get propotion of each imcome bracket by dividing by total\n",
    "inc_all = create_proportions(inc_all, numeric_vars, \"total\")\n",
    "# Match Crime data and Get rid of MSA\n",
    "inc_all = match_crime(inc_all)\n",
    "del inc_all['MSA']\n",
    "inc_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# GINI INDEX\n",
    "###############\n",
    "gini_all = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    f = get_file_name(year, 'B19083')\n",
    "    gini = pd.read_csv(\"data/gini/%s.csv\" %f, encoding='Latin-1')\n",
    "    # Don't need micro areas\n",
    "    gini = slim_df(gini, [\"HD01_VD01\"])\n",
    "    gini['year'] = year\n",
    "    gini_all = append_df(gini, gini_all) \n",
    "\n",
    "# Clean Final Dataframes\n",
    "gini_all = gini_all.rename(index=str,\n",
    "                           columns={\"HD01_VD01\":\"gini\"})\n",
    "gini_all['gini'] = gini_all['gini'].astype(float)\n",
    "gini_all = match_crime(gini_all)\n",
    "del gini_all['MSA']\n",
    "gini_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Poverty Data\n",
    "#################\n",
    "pov_all = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    f = get_file_name(year, 'S1701')\n",
    "    pov = pd.read_csv(\"data/poverty/%s.csv\" %f, encoding='Latin-1')\n",
    "    pov = slim_df(pov, ['HC03_EST_VC03', 'HC03_EST_VC05',\n",
    "                       'HC03_EST_VC08', 'HC03_EST_VC09'])\n",
    "    pov['year'] = year\n",
    "    pov_all = append_df(pov, pov_all)\n",
    "# Clean Final DataFrame\n",
    "pov_all = pov_all.rename(index=str,\n",
    "                        columns={'HC03_EST_VC03': 'under_18_pov',\n",
    "                                'HC03_EST_VC05':'18_64_pov',\n",
    "                                'HC03_EST_VC08':'male_pov',\n",
    "                                'HC03_EST_VC09':'female_pov'})\n",
    "pov_all = match_crime(pov_all)\n",
    "del pov_all['MSA']\n",
    "pov_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Head of Household Information\n",
    "#################################\n",
    "house_all = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    f = get_file_name(year, 'B09005')\n",
    "    house = pd.read_csv(\"data/house_head/%s.csv\" %f, encoding='Latin-1')\n",
    "    house = slim_df(house, [v for v in house.columns if \"HD01\" in v])\n",
    "    house = house.loc[:, [\"MSA\", \"city_key\", \"state_key\",\n",
    "                         \"HD01_VD01\", \"HD01_VD03\", \"HD01_VD05\",\n",
    "                         \"HD01_VD06\"]]\n",
    "    house['year'] = year\n",
    "    house_all = append_df(house, house_all) \n",
    "\n",
    "# Clean Entire DataFrame\n",
    "house_all = house_all.rename(index=str,\n",
    "                             columns={'HD01_VD01': 'total',\n",
    "                                     'HD01_VD03': 'married_house',\n",
    "                                     'HD01_VD05': 'female_house',\n",
    "                                     'HD01_VD06': 'male_house'})\n",
    "\n",
    "house_all = convert_to_int(house_all,\n",
    "                       ['total', 'married_house', 'female_house', 'male_house'])\n",
    "house_all = create_proportions(house_all, ['married_house', 'female_house', 'male_house'], 'total')\n",
    "house_all = match_crime(house_all)\n",
    "del house_all['MSA']\n",
    "house_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Education Data\n",
    "#################\n",
    "edu_all = pd.DataFrame()\n",
    "for year in range(2006,2017):\n",
    "    f = get_file_name(year, 'S1501')\n",
    "    edu = pd.read_csv(\"data/education/%s.csv\" %f, encoding='Latin-1')\n",
    "    if 2015 <= year <= 2016:\n",
    "        edu = slim_df(edu, ['HC02_EST_VC03', 'HC02_EST_VC04', 'HC02_EST_VC09','HC02_EST_VC10', 'HC02_EST_VC11'])\n",
    "    elif 2010 <= year <= 2014:\n",
    "        edu = slim_df(edu,['HC01_EST_VC02', 'HC01_EST_VC03', 'HC01_EST_VC08','HC01_EST_VC09', 'HC01_EST_VC10'])\n",
    "    else:\n",
    "        edu = slim_df(edu, ['HC01_EST_VC02', 'HC01_EST_VC03', 'HC01_EST_VC07','HC01_EST_VC08', 'HC01_EST_VC09'])\n",
    "    \n",
    "    edu['year'] = year\n",
    "    edu.columns=['no_hs_18_24','hs_18_24','no_9th_25_ovr','no_hs_25_ovr','hs_25_ovr','MSA','city_key','state_key','year']\n",
    "    edu_all = append_df(edu, edu_all)\n",
    "\n",
    "#Trim Final Dataframe\n",
    "edu_all = match_crime(edu_all)\n",
    "del edu_all['MSA']\n",
    "edu_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Race Data\n",
    "############\n",
    "race_all = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    f = get_file_name(year, 'B02001')\n",
    "    race = pd.read_csv(\"data/race/%s.csv\" %f, encoding='Latin-1')\n",
    "    race = slim_df(race, [v for v in race.columns if \"HD01\" in v])\n",
    "    race = race.loc[:, ['MSA', 'city_key', 'state_key',\n",
    "                       'HD01_VD01','HD01_VD02',\n",
    "                       'HD01_VD03', 'HD01_VD05']]\n",
    "    \n",
    "    race['year'] = year\n",
    "    race_all = append_df(race, race_all)\n",
    "\n",
    "# Proccess Final Data Frame\n",
    "race_all =  race_all.rename(index=str,\n",
    "                            columns={'HD01_VD01':'total',\n",
    "                                    'HD01_VD02': 'white',\n",
    "                                    'HD01_VD03': 'black',\n",
    "                                    'HD01_VD05': 'asian'})\n",
    "\n",
    "race_all = convert_to_int(race_all, ['total', 'white','black','asian'] )\n",
    "race_all = create_proportions(race_all, ['white', 'black', 'asian'], 'total')\n",
    "# Match Crime Data\n",
    "race_all = match_crime(race_all)\n",
    "del race_all['MSA']\n",
    "race_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bring Everything Together\n",
    "census_df = race_all.copy()\n",
    "\n",
    "merge_df = lambda df: census_df.merge(df,\n",
    "                                     how='outer',\n",
    "                                     on=['city_key','state_key','year'],\n",
    "                                     indicator=True)\n",
    "\n",
    "str_list = ['Employment', 'Age', 'Head of House','Education', 'Gini', 'Poverty']\n",
    "df_list = [emp_all, age_all, house_all, edu_all, gini_all, pov_all]\n",
    "\n",
    "for i, df in enumerate(df_list):\n",
    "    census_df = merge_df(df)\n",
    "    #print(\"%s Merge Stats\" %str_list[i])\n",
    "    #print(census_df['_merge'].value_counts())\n",
    "    del census_df['_merge']\n",
    "    \n",
    "'''\n",
    "\n",
    "Household had ones where it looks like there may be mismatches but code below checked it\n",
    "\n",
    "Code to check merges\n",
    "names = census_df.loc[census_df._merge != \"both\", ['city_key', 'state_key', '_merge']]\n",
    "names = names.sort_values['city_key', 'state_key']\n",
    "names = names.drop_duplicates()\n",
    "print(names.shape[0])\n",
    "names.head(50)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Bring in BEA Data\n",
    "#####################\n",
    "bea_gdp = pd.read_csv(\"data/BEA_real_GDP_pc.csv\",skiprows=[0,1,2], header=1)\n",
    "del bea_gdp['Fips']\n",
    "bea_gdp= bea_gdp.iloc[1:, :].rename(index=str, columns={\"Area\": 'MSA'})\n",
    "bea_gdp = pd.melt(bea_gdp, id_vars=[\"MSA\"], var_name='year', value_name='real_pc_gdp')\n",
    "bea_gdp = bea_gdp.loc[bea_gdp.MSA.notnull(), :]\n",
    "bea_gdp['year'] = bea_gdp['year'].astype(int)\n",
    "bea_gdp = bea_gdp.loc[bea_gdp.year >= 2006, :]\n",
    "\n",
    "# Get rid of MSA in paranthesis\n",
    "bea_gdp['MSA'] = bea_gdp['MSA'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "bea_gdp = split_MSA(bea_gdp)\n",
    "# Need to manually fix this one so it will merge\n",
    "bea_gdp.loc[bea_gdp.city_key.str.contains(\"Louisville\"), 'city_key'] = 'Louisville'\n",
    "\n",
    "del bea_gdp['MSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df = merge_df(bea_gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure that there were no typos\n",
    "names = census_df.loc[census_df._merge != \"both\", ['city_key', 'state_key', '_merge']]\n",
    "names = names.drop_duplicates()\n",
    "print(names.shape[0])\n",
    "del census_df[\"_merge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "census_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE ONLY NEEDS TO BE RUN ONCE TO BRING IN ALL OF THE EXCEL FILES\n",
    "version='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36'\n",
    "test=urllib.request.URLopener()\n",
    "test.addheader('User-Agent',version)\n",
    "for year in range(2006, 2017):\n",
    "    print(\"Pulling: %i\" %year)\n",
    "    test.retrieve(url=fbi_url_generator(year),filename='crime_%i.xls' %year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyears = pd.DataFrame()\n",
    "for year in range(2006, 2017):\n",
    "    df = pd.read_excel(\"crime_%i.xls\" %year,skiprows=[0,1],header=1)\n",
    "\n",
    "    #######\n",
    "    # NOTE - misc column has msa population, city population and estimate percentage\n",
    "    ######\n",
    "    df=df.iloc[:,0:12] \n",
    "    df.columns=['MSA', 'counties','misc', 'violent_crime','mur_mans', 'rape', 'robbery',\n",
    "                'assault', 'property', 'burglary', 'larceny','mv_theft']\n",
    "    \n",
    "    df['counties'].replace(' ',np.nan, inplace=True)\n",
    "\n",
    "    # Drop footnotes\n",
    "    footnotes = df['MSA'].str[0].str.isdigit().fillna(False)\n",
    "    df = df.loc[~footnotes, :]\n",
    "    \n",
    "    #Drop blank rows\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    # Get rid of numbers in MSA\n",
    "    df['MSA'] = df['MSA'].str.replace('\\d+', '')\n",
    "    # Set empty columns to NaN for MSA\n",
    "    df['MSA'] = df['MSA'].replace(' ', np.nan, regex=False)\n",
    "    \n",
    "    # Sometimes city  names get put in MSA column\n",
    "    # Messes up carry forward\n",
    "    df.loc[df['MSA'].str.contains(\"City of\").fillna(False), \"MSA\"] = np.nan\n",
    "\n",
    "    # Carry MSA name forward to fill in for all cells\n",
    "    df.loc[:,'MSA'] = df.loc[:, 'MSA'].fillna(method='ffill')\n",
    "\n",
    "    ##############\n",
    "    # POPULATION - grab population and fill in for all MSA\n",
    "    ##############\n",
    "    pop_row = df.counties.isnull()\n",
    "    pop = df.loc[pop_row, [\"MSA\", 'misc']]\n",
    "    pop = pop.rename(index=str, columns={'misc': 'msa_pop'})\n",
    "    \n",
    "    # Merge population back in\n",
    "    df = df.loc[~pop_row, :]\n",
    "    df = df.merge(pop, how='outer', on='MSA')\n",
    "\n",
    "    ################\n",
    "    # Descriptions - don't need county descriptions \n",
    "    ################\n",
    "    df = df.loc[df.counties.str.contains(\"Includes\") == False, :]\n",
    "\n",
    "\n",
    "    ###########################################\n",
    "    # GOING LONG TO WIDE FOR CRIME VARIABLES\n",
    "    ###########################################\n",
    "    crime_vars = ['violent_crime','mur_mans', 'rape', 'robbery',\n",
    "                  'assault', 'property', 'burglary', 'larceny','mv_theft']\n",
    "\n",
    "    #########\n",
    "    # CITIES\n",
    "    #########\n",
    "    city_vars = ['MSA', 'counties', 'misc'] + crime_vars\n",
    "    # Split data Frame\n",
    "    cities = df.counties.str.contains(\"City\")\n",
    "    city_df = df.loc[cities, city_vars]\n",
    "    city_df = city_df.rename(index=str, columns={'misc': 'city_pop'})\n",
    "    \n",
    "    # Grab largest city for each MSA and merge back on\n",
    "    city_df = city_df.sort_values(['MSA','city_pop'], ascending=False)\n",
    "    large_city = city_df.groupby('MSA').first().reset_index()\n",
    "\n",
    "    # Rename crime variables to denote city only crime \n",
    "    large_city.columns = ['MSA', 'counties', 'city_pop'] + ['city_' + i for i in crime_vars]\n",
    "    large_city = large_city.rename(index=str, columns={'counties':'largest_city'})\n",
    "    # Get rid of \"City of\"\n",
    "    large_city.loc[:,'largest_city'] = large_city.loc[:, 'largest_city'].str.replace('City of','')\n",
    "    \n",
    "    # Merge back to main dataframe\n",
    "    df = df.loc[~cities, ]\n",
    "    df = df.merge(large_city, how='outer', on='MSA')\n",
    "\n",
    "    ###############\n",
    "    # CRIME RATE\n",
    "    ###############\n",
    "    rates = df.counties.str.contains(\"Rate per\")\n",
    "    rate_vars = ['MSA'] + crime_vars\n",
    "    rates_df = df.loc[rates, rate_vars]\n",
    "    rates_df.columns = ['MSA'] + ['rate_' + i for i in crime_vars]\n",
    "\n",
    "    df = df.loc[~rates, :]\n",
    "    df = df.merge(rates_df, how='outer', on='MSA')\n",
    "\n",
    "    ########################\n",
    "    # MSA-WIDE CRIME STATS\n",
    "    ########################\n",
    "\n",
    "    # If the entire MSA reported then there is just one row of numbers\n",
    "    # If the entire MSA did not report, then there are two rows\n",
    "            # first row is areas that reported\n",
    "            # second report is an estimated total\n",
    "    # We are going to grab the estimates total so our data\n",
    "    # reflects all areas for all MSA\n",
    "\n",
    "    # Create Flag for those that do not have complete coverage\n",
    "    # and are thus estimates\n",
    "    mins = df.groupby('MSA').misc.min().reset_index()\n",
    "    mins.columns = ['MSA', 'min_coverage']\n",
    "    df = df.merge(mins, how='outer', on='MSA')\n",
    "    df['estimate'] = 0\n",
    "    df.loc[df.min_coverage < 1, 'estimate'] = 1\n",
    "    del df['min_coverage']\n",
    "\n",
    "    # Now only keeping rows with coverage = 1\n",
    "    # will either be all area or the estimate for all area\n",
    "    df = df.loc[df.misc == 1, :]\n",
    "\n",
    "    # Now no longer need coverage or whether its estimate or not\n",
    "    del df['misc']\n",
    "    del df['counties']\n",
    "    \n",
    "    df['year'] = year\n",
    "    \n",
    "    # Append to existing Frame\n",
    "    df_allyears = append_df(df, df_allyears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyears = df_allyears.sort_values([\"MSA\", 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate the city\n",
    "df_allyears['city_key'] = df_allyears['MSA'].str.replace(' M.S.A.','').str.split(\",\").str[0]\n",
    "df_allyears['city_key'] = df_allyears['city_key'].str.split(\"-\").str[0].str.strip()\n",
    "df_allyears['state_key']=df_allyears['MSA'].str.replace(' M.S.A.','').str.split(\",\").str[1].str.strip().str.replace(' M.S.A','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Cleanse Crime Data\n",
    "df_allyears=df_allyears[~df_allyears['MSA'].str.contains(' M.D.')]\n",
    "df_allyears.loc[df_allyears['state_key']=='Puerto Rico','state_key']='PR'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Texarkana'),'state_key']='AR-TX'\n",
    "df_allyears.loc[df_allyears['city_key']=='Worcester','state_key']='MA-CT'\n",
    "df_allyears.loc[df_allyears['city_key']=='Steubenville','city_key']='Weirton'\n",
    "df_allyears.loc[df_allyears['city_key']=='Steubenville','state_key']='WV-OH'\n",
    "df_allyears.loc[df_allyears['city_key']=='Honolulu','city_key']='Urban Honolulu'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Scranton'),'city_key']='Scranton'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Sarasota'),'city_key']='North Port'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Santa Maria'),'city_key']='Santa Maria'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Salisbury'),'state_key']='MD-DE'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Sacramento'),'city_key']='Sacramento'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Myrtle Beach'),'state_key']='SC-NC'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Louisville'),'city_key']='Louisville'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Homosassa'),'city_key']='Homosassa Springs'\n",
    "df_allyears.loc[df_allyears['MSA'].str.contains('Crestview'),'city_key']='Crestview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Crime and census data\n",
    "final_df = df_allyears.merge(census_df, how='left', on=['city_key','state_key','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert to float otherwise set to NaN\n",
    "def f(x):\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at final Cleaning of data-types\n",
    "float_cols = final_df.columns.difference([\"MSA\", \"city_key\", \"state_key\",\"year\", \"largest_city\"])\n",
    "for v in float_cols:\n",
    "    try:\n",
    "        final_df[v] = final_df[v].astype(float)\n",
    "    except:\n",
    "        continue\n",
    "# Assault Rate Needs to be fixed\n",
    "# One Value is missing\n",
    "final_df.loc[final_df.rate_assault == \" \", 'rate_assault'] = np.nan\n",
    "final_df[\"rate_assault\"] = final_df[\"rate_assault\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a join key\n",
    "final_df['join_key'] = final_df['city_key'].str.cat(final_df['state_key'],sep='-')\n",
    "#Add columns for OHE\n",
    "final_df['year_ohe'] = final_df['year']\n",
    "final_df['state_ohe'] = final_df['state_key']\n",
    "final_df['join_ohe'] = final_df['join_key']\n",
    "#One hot encode join key, state key and year\n",
    "final_df = pd.get_dummies(final_df,prefix='year',columns=['year_ohe']) #Not dropping one column since year has missing values\n",
    "final_df = pd.get_dummies(final_df,prefix=['MSA','state'],columns=['join_ohe','state_ohe'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_json('output/final.json')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
